{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(6).reshape(3, 2)\n",
    "mask = torch.tensor([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "]).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normal masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 3],\n",
       "        [4, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(6).reshape(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking and filling in with a single value\n",
    "\n",
    "This is often used for padding tensors with `FILL_VALUE` as the padding value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 2, -1],\n",
       "        [-1,  5]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILL_VALUE = -1\n",
    "x.masked_fill(mask, PAD_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill with a single value along a dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1, -1],\n",
       "        [ 2,  3],\n",
       "        [-1, -1]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([0, 2])\n",
    "dim = 0\n",
    "x.index_fill(dim=dim, index=idx, value=FILL_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put multiple entries of a tensor in original tensor `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1],\n",
       "        [  2, -10],\n",
       "        [-20,   5]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tensor([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "]).bool()\n",
    "values = torch.tensor([-10, -20])\n",
    "\n",
    "x[mask] = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `x[mask] = values` is in-place! Behind the scenes, this converts the mask into a tuple of indexing tensors, then calls `index_put_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling a mask with a fixed value using scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.zeros(2, 3)\n",
    "idxs = torch.tensor([\n",
    "    [0, 2],\n",
    "    [1, 1]\n",
    "])\n",
    "dim = 1\n",
    "value = 1.0\n",
    "mask.scatter(dim, idxs, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling a tensor with different values using scatter\n",
    "\n",
    "`idx` and `src` should have the same shape\n",
    "`idx` and `target` should have same dimensions everywhere except along `dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.,  0., 30.],\n",
       "        [ 0., 20.,  0.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.zeros(2, 3)\n",
    "idxs = torch.tensor([\n",
    "    [0, 2],\n",
    "    [1, 1]\n",
    "])\n",
    "dim = 1\n",
    "src = torch.tensor([\n",
    "    [10., 30.],\n",
    "    [20., 20.],\n",
    "])\n",
    "target.scatter(dim, idxs, src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filling a tensor with masked scatter (scatter into places where mask is true)\n",
    "\n",
    "Note that `source` should be just a 1d tensor, but it can have more values than number of true entries in `mask`\n",
    "If `source` is not a 2d tensor, masked_scatter will flatten `source` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 2, 10],\n",
       "        [20,  5]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6).reshape(3, 2)\n",
    "mask = torch.tensor([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "]).bool()\n",
    "\n",
    "source = torch.tensor([\n",
    "    10, 20, 30, 40, 50\n",
    "])\n",
    "\n",
    "x.masked_scatter(mask, source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Select and indexing\n",
    "\n",
    "Some terminology: \n",
    "\n",
    "- By \"entry\" we mean a position in the tensor, like position `(i,j)`\n",
    "\n",
    "- By \"value\" we mean the data in a particular entry, like `tsr[i,j])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6).reshape(3, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select along a particular dimension\n",
    "\n",
    "Each entry of `idxs` corresponds to an entry along `dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [4, 5],\n",
       "        [0, 1],\n",
       "        [0, 1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 0\n",
    "idxs = torch.tensor([0, 2, 0, 0])\n",
    "x.index_select(dim=dim, index=idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select along a particular dimension (equivalent, via fancy indexing)\n",
    "\n",
    "Each entry of `idxs` corresponds to an entry along `dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [4, 5],\n",
       "        [0, 1],\n",
       "        [0, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = torch.tensor([0, 2, 0, 0])\n",
    "x[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select along a particular dimension (equivalent, via fancy indexing, but now along columns)\n",
    "\n",
    "Each entry of `idxs` corresponds to an entry along `dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [3, 2, 2],\n",
       "        [5, 4, 4]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = torch.tensor([1, 0, 0])\n",
    "x[:, idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting according to entries of a mask\n",
    "\n",
    "`torch.masked_select` has the following signature type:\n",
    "\n",
    "```\n",
    "torch.masked_select(\n",
    "    input: TensorType[D1, D2, D3, ..., DN],\n",
    "    mask: TensorType[D1, D2, D3, ..., DN]\n",
    "): -> out: TensorType[D]\n",
    "```\n",
    "\n",
    "`out` will contain the entries of `input` where `mask` is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tensor([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "]).bool()\n",
    "x.masked_select(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting according to a mask but using fancy indexing\n",
    "\n",
    "Use masked select if you just want a list of entries.\n",
    "If you want to keep the shape of the original tensor `x`, you should use normal masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tensor([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "]).bool()\n",
    "x[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting so that `out[i,j] = input[row[i, j], cols[i, j]]`\n",
    "\n",
    "`out.shape` will be the same as `row.shape`.\n",
    "Also `row.shape` must be equal to `cols.shape` (or at least broadcastable, see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5, 4],\n",
       "        [0, 1, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = torch.tensor([\n",
    "    [2, 2, 2],\n",
    "    [0, 0, 0],\n",
    "])\n",
    "cols = torch.tensor([\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 0],\n",
    "])\n",
    "x[rows, cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting so that out[i,j] = input[row[i], cols[j]] (recommended way using broadcasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5, 4],\n",
       "        [0, 1, 0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = torch.tensor([2, 0])\n",
    "cols = torch.tensor([0, 1, 0])\n",
    "x[rows.unsqueeze(1), cols.unsqueeze(0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `x[rows.unsqueeze(1), cols.unsqueeze(0)]` broadcasts `rows` and `cols` to be the same shape as each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert a mask into indices (list of nonzero entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [2, 0]])\n",
      "(tensor([1, 2]), tensor([1, 0]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3, 4])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tensor([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "]).bool()\n",
    "nonzeros = mask.nonzero()\n",
    "print(nonzeros)\n",
    "\n",
    "# then indexing using `nonzeros` by turning it into a pair of list of indices\n",
    "pair = tuple(nonzeros.T)\n",
    "print(pair)\n",
    "x[pair]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting with a single Long tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each value in `tsr` will be read as an entry along dim 0 of `x`,\n",
    "so that the resulting shape is `(*tsr.shape, x.shape[1:])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 3],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[4, 5],\n",
       "         [0, 1]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsr = torch.tensor([\n",
    "    [1, 1],\n",
    "    [2, 0],\n",
    "])\n",
    "x[tsr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Selecting with a single Long tensor (equivalent, but don't do this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 3],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[4, 5],\n",
       "         [0, 1]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[tsr.flatten()].reshape(*tsr.shape, x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Indexing with slices `:` and ellipsises `...`\n",
    "Note that pytorch may be slower with slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Indexing with slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1, 0, 0],\n",
       "        [3, 2, 3, 2, 2],\n",
       "        [5, 4, 5, 4, 4]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = [1, 0, 1, 0, 0]\n",
    "x[:, idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to indexing with a tuple of indexing arrays that get broadcasted.\n",
    "Think about slicing as equivalent to indexing with `torch.arange`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1, 0, 0],\n",
       "        [3, 2, 3, 2, 2],\n",
       "        [5, 4, 5, 4, 4]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[torch.arange(x.size(0)).unsqueeze(1), torch.tensor(idxs).unsqueeze(0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Indexing using gather\n",
    "\n",
    "Think about gather as a multidimensional version of selecting with broadcasting a tuple of indexing arrays.\n",
    "Consider:\n",
    "```python\n",
    "x.shape == (N x D)\n",
    "x[torch.arange(N).unsqueeze(1), torch.tensor([1, 2].unsqueeze(0))]\n",
    "```\n",
    "Here we gather along `dim==1`, resulting in an output shape of `N x 2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we had a `B x T x D` tensor representing a batch of RNN outputs each with dimension `D`\n",
    "Let's say we wanted to select only the last timestep of each sequence of each batch\n",
    "and that we knew how long each sequence was for each batch. We can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 1, 1, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[5., 6., 7., 8.]],\n",
       "\n",
       "        [[9., 8., 7., 6.]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsr = torch.tensor([\n",
    "    [[1., 2., 3., 4.],\n",
    "     [5., 6., 7., 8.],\n",
    "     [0., 0., 0., 0.],\n",
    "    ],\n",
    "    [[9., 8., 7., 6.],\n",
    "     [0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0.],\n",
    "    ],\n",
    "]) # use 0 to indicate padding\n",
    "N, T, D = tsr.shape\n",
    "\n",
    "lengths = torch.tensor([2, 1])  # length of each of the N sequences\n",
    "\n",
    "last_idxs = lengths - 1\n",
    "index = last_idxs.unsqueeze(-1).expand(N, D).unsqueeze(-2)  # B x 1 x D\n",
    "print(index)\n",
    "tsr.gather(index=index, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output size will be the same as `index` size in all dimensions except `dim`.\n",
    "`index` must have the same size in all dimensions as `tsr` except dimension `dim`.\n",
    "`dim` specifies the dimension along which the values in `index` will be indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Indexing timings\n",
    "\n",
    "Let's see which of these equivalent gather methods are faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gather(tensor, indices):\n",
    "    output = []\n",
    "    for i in range(tensor.size(0)):\n",
    "        output += [tensor[i][indices[i]]]\n",
    "    return torch.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gather_vec(tensor, indices):\n",
    "    shape = list(tensor.shape)\n",
    "    flat_first = torch.reshape(\n",
    "        tensor, [shape[0] * shape[1]] + shape[2:])\n",
    "    offset = torch.reshape(\n",
    "        torch.arange(shape[0], device=tensor.device) * shape[1],\n",
    "        [shape[0]] + [1] * (len(indices.shape) - 1))\n",
    "    output = flat_first[indices + offset]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "x = torch.randn(1000, 200, device=device)\n",
    "idx = torch.randint(200, (1000, ), device=device)\n",
    "idx_lst = idx.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.6 ms ± 36.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch_gather(x, idx).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.4 µs ± 1.2 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch_gather_vec(x, idx).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6 µs ± 332 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x.gather(index=idx.unsqueeze(-1), dim=-1).squeeze(-1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the gather function built into pytorch is the fastest.\n",
    "\n",
    "How about the timing for summing along an axis/dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.95 ms ± 50.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "res = 0\n",
    "for i in range(len(x)):\n",
    "    res = res + x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 ms ± 39.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "res = 0\n",
    "for row in x:\n",
    "    res = res + row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.99 ms ± 26.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "res = 0\n",
    "for row in x.unbind():\n",
    "    res = res + row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 µs ± 17.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about indexing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.3 µs ± 568 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x[idx_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.03 µs ± 9.33 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x.index_select(index=idx, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.39 µs ± 62.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x[idx]"
   ]
  }
 ],
 "metadata": {
  "author": [
   "Ke Alexander Wang"
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "title": "Advanced masking, filling, selecting, and indexing PyTorch tensors"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
