<!DOCTYPE html>
<!--
==============================================================================
           "GitHub HTML5 Pandoc Template" v2.2 — by Tristano Ajmone
==============================================================================
Copyright © Tristano Ajmone, 2017-2020, MIT License (MIT). Project's home:
- https://github.com/tajmone/pandoc-goodies
The CSS in this template reuses source code taken from the following projects:
- GitHub Markdown CSS: Copyright © Sindre Sorhus, MIT License (MIT):
  https://github.com/sindresorhus/github-markdown-css
- Primer CSS: Copyright © 2016-2017 GitHub Inc., MIT License (MIT):
  http://primercss.io/
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The MIT License
Copyright (c) Tristano Ajmone, 2017-2020 (github.com/tajmone/pandoc-goodies)
Copyright (c) Sindre Sorhus <sindresorhus@gmail.com> (sindresorhus.com)
Copyright (c) 2017 GitHub Inc.
"GitHub Pandoc HTML5 Template" is Copyright (c) Tristano Ajmone, 2017-2020,
released under the MIT License (MIT); it contains readaptations of substantial
portions of the following third party softwares:
(1) "GitHub Markdown CSS", Copyright (c) Sindre Sorhus, MIT License (MIT).
(2) "Primer CSS", Copyright (c) 2016 GitHub Inc., MIT License (MIT).
Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
==============================================================================-->
<html>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>advanced-torch-tensors</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../../stylesheet.css">
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article class="markdown-body">
<header>
<h1 class="title">Advanced masking, filling, selecting, and indexing
PyTorch tensors</h1>
<p class="author">Ke Alexander Wang</p>
</header>
<hr>
<nav id="TOC">
<h1 class="toc-title">Contents</h1>
<ul>
<li><a href="#masking">Masking</a></li>
<li><a href="#normal-masking">Normal masking</a></li>
<li><a href="#filling">Filling</a></li>
<li><a href="#masking-and-filling-in-with-a-single-value">Masking and
filling in with a single value</a></li>
<li><a href="#fill-with-a-single-value-along-a-dimension">Fill with a
single value along a dimension</a></li>
<li><a href="#put-multiple-entries-of-a-tensor-in-original-tensor-x">Put
multiple entries of a tensor in original tensor <code>x</code></a></li>
<li><a href="#filling-a-mask-with-a-fixed-value-using-scatter">Filling a
mask with a fixed value using scatter</a></li>
<li><a
href="#filling-a-tensor-with-different-values-using-scatter">Filling a
tensor with different values using scatter</a></li>
<li><a
href="#filling-a-tensor-with-masked-scatter-scatter-into-places-where-mask-is-true">Filling
a tensor with masked scatter (scatter into places where mask is
true)</a></li>
<li><a href="#select-and-indexing">Select and indexing</a></li>
<li><a href="#select-along-a-particular-dimension">Select along a
particular dimension</a></li>
<li><a
href="#select-along-a-particular-dimension-equivalent-via-fancy-indexing">Select
along a particular dimension (equivalent, via fancy indexing)</a></li>
<li><a
href="#select-along-a-particular-dimension-equivalent-via-fancy-indexing-but-now-along-columns">Select
along a particular dimension (equivalent, via fancy indexing, but now
along columns)</a></li>
<li><a href="#selecting-according-to-entries-of-a-mask">Selecting
according to entries of a mask</a></li>
<li><a
href="#selecting-according-to-a-mask-but-using-fancy-indexing">Selecting
according to a mask but using fancy indexing</a></li>
<li><a href="#selecting-so-that-outij--inputrowi-j-colsi-j">Selecting so
that <code>out[i,j] = input[row[i, j], cols[i, j]]</code></a></li>
<li><a
href="#selecting-so-that-outij--inputrowi-colsj-recommended-way-using-broadcasting">Selecting
so that out[i,j] = input[row[i], cols[j]] (recommended way using
broadcasting)</a></li>
<li><a
href="#convert-a-mask-into-indices-list-of-nonzero-entries">Convert a
mask into indices (list of nonzero entries)</a></li>
<li><a href="#selecting-with-a-single-long-tensor">Selecting with a
single Long tensor</a></li>
<li><a
href="#selecting-with-a-single-long-tensor-equivalent-but-dont-do-this">Selecting
with a single Long tensor (equivalent, but don't do this!)</a></li>
<li><a href="#indexing-with-slices--and-ellipsises-">Indexing with
slices <code>:</code> and ellipsises <code>...</code></a></li>
<li><a href="#indexing-with-slicing">Indexing with slicing</a></li>
<li><a href="#indexing-using-gather">Indexing using gather</a></li>
<li><a href="#indexing-timings">Indexing timings</a></li>
</ul>
</nav>
<hr>
<div class="cell code" data-execution_count="1">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span></code></pre></div>
</div>
<section id="masking" class="cell markdown" data-tags="[]">
<h1>Masking</h1>
</section>
<div class="cell code" data-execution_count="20">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="dv">6</span>).reshape(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.tensor([</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>]).<span class="bu">bool</span>()</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="21">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x</span></code></pre></div>
<div class="output execute_result" data-execution_count="21">
<pre><code>tensor([[0, 1],
        [2, 3],
        [4, 5]])</code></pre>
</div>
</div>
<section id="normal-masking" class="cell markdown" data-tags="[]">
<h3>Normal masking</h3>
</section>
<div class="cell code" data-execution_count="22">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">*</span> mask</span></code></pre></div>
<div class="output execute_result" data-execution_count="22">
<pre><code>tensor([[0, 0],
        [0, 3],
        [4, 0]])</code></pre>
</div>
</div>
<section id="filling" class="cell markdown" data-tags="[]">
<h1>Filling</h1>
</section>
<div class="cell code" data-execution_count="45">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="dv">6</span>).reshape(<span class="dv">3</span>, <span class="dv">2</span>)</span></code></pre></div>
</div>
<section id="masking-and-filling-in-with-a-single-value"
class="cell markdown">
<h2>Masking and filling in with a single value</h2>
<p>This is often used for padding tensors with <code>FILL_VALUE</code>
as the padding value</p>
</section>
<div class="cell code" data-execution_count="46">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>FILL_VALUE <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>x.masked_fill(mask, PAD_VALUE)</span></code></pre></div>
<div class="output execute_result" data-execution_count="46">
<pre><code>tensor([[ 0,  1],
        [ 2, -1],
        [-1,  5]])</code></pre>
</div>
</div>
<section id="fill-with-a-single-value-along-a-dimension"
class="cell markdown">
<h2>Fill with a single value along a dimension</h2>
</section>
<div class="cell code" data-execution_count="48">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> torch.tensor([<span class="dv">0</span>, <span class="dv">2</span>])</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>x.index_fill(dim<span class="op">=</span>dim, index<span class="op">=</span>idx, value<span class="op">=</span>FILL_VALUE)</span></code></pre></div>
<div class="output execute_result" data-execution_count="48">
<pre><code>tensor([[-1, -1],
        [ 2,  3],
        [-1, -1]])</code></pre>
</div>
</div>
<section id="put-multiple-entries-of-a-tensor-in-original-tensor-x"
class="cell markdown">
<h2>Put multiple entries of a tensor in original tensor
<code>x</code></h2>
</section>
<div class="cell code" data-execution_count="51">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.tensor([</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>]).<span class="bu">bool</span>()</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> torch.tensor([<span class="op">-</span><span class="dv">10</span>, <span class="op">-</span><span class="dv">20</span>])</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>x[mask] <span class="op">=</span> values</span></code></pre></div>
<div class="output execute_result" data-execution_count="51">
<pre><code>tensor([[  0,   1],
        [  2, -10],
        [-20,   5]])</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Note that <code>x[mask] = values</code> is in-place! Behind the
scenes, this converts the mask into a tuple of indexing tensors, then
calls <code>index_put_</code></p>
</div>
<section id="filling-a-mask-with-a-fixed-value-using-scatter"
class="cell markdown">
<h2>Filling a mask with a fixed value using scatter</h2>
</section>
<div class="cell code" data-execution_count="116">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.zeros(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> torch.tensor([</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">2</span>],</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>value <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>mask.scatter(dim, idxs, value)</span></code></pre></div>
<div class="output execute_result" data-execution_count="116">
<pre><code>tensor([[1., 0., 1.],
        [0., 1., 0.]])</code></pre>
</div>
</div>
<section id="filling-a-tensor-with-different-values-using-scatter"
class="cell markdown">
<h2>Filling a tensor with different values using scatter</h2>
<p><code>idx</code> and <code>src</code> should have the same shape
<code>idx</code> and <code>target</code> should have same dimensions
everywhere except along <code>dim</code></p>
</section>
<div class="cell code" data-execution_count="118">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> torch.zeros(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> torch.tensor([</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">2</span>],</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> torch.tensor([</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">10.</span>, <span class="fl">30.</span>],</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">20.</span>, <span class="fl">20.</span>],</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>target.scatter(dim, idxs, src)</span></code></pre></div>
<div class="output execute_result" data-execution_count="118">
<pre><code>tensor([[10.,  0., 30.],
        [ 0., 20.,  0.]])</code></pre>
</div>
</div>
<section
id="filling-a-tensor-with-masked-scatter-scatter-into-places-where-mask-is-true"
class="cell markdown" data-tags="[]">
<h2>Filling a tensor with masked scatter (scatter into places where mask
is true)</h2>
<p>Note that <code>source</code> should be just a 1d tensor, but it can
have more values than number of true entries in <code>mask</code> If
<code>source</code> is not a 2d tensor, masked_scatter will flatten
<code>source</code> first</p>
</section>
<div class="cell code" data-execution_count="123">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="dv">6</span>).reshape(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.tensor([</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>]).<span class="bu">bool</span>()</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>source <span class="op">=</span> torch.tensor([</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">40</span>, <span class="dv">50</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>x.masked_scatter(mask, source)</span></code></pre></div>
<div class="output execute_result" data-execution_count="123">
<pre><code>tensor([[ 0,  1],
        [ 2, 10],
        [20,  5]])</code></pre>
</div>
</div>
<section id="select-and-indexing" class="cell markdown" data-tags="[]">
<h1>Select and indexing</h1>
<p>Some terminology:</p>
<ul>
<li><p>By "entry" we mean a position in the tensor, like position
<code>(i,j)</code></p></li>
<li><p>By "value" we mean the data in a particular entry, like
<code>tsr[i,j])</code></p></li>
</ul>
</section>
<div class="cell code" data-execution_count="53">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="dv">6</span>).reshape(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>x</span></code></pre></div>
<div class="output execute_result" data-execution_count="53">
<pre><code>tensor([[0, 1],
        [2, 3],
        [4, 5]])</code></pre>
</div>
</div>
<section id="select-along-a-particular-dimension" class="cell markdown">
<h2>Select along a particular dimension</h2>
<p>Each entry of <code>idxs</code> corresponds to an entry along
<code>dim</code></p>
</section>
<div class="cell code" data-execution_count="31">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> torch.tensor([<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>x.index_select(dim<span class="op">=</span>dim, index<span class="op">=</span>idxs)</span></code></pre></div>
<div class="output execute_result" data-execution_count="31">
<pre><code>tensor([[0, 1],
        [4, 5],
        [0, 1],
        [0, 1]])</code></pre>
</div>
</div>
<section
id="select-along-a-particular-dimension-equivalent-via-fancy-indexing"
class="cell markdown">
<h2>Select along a particular dimension (equivalent, via fancy
indexing)</h2>
<p>Each entry of <code>idxs</code> corresponds to an entry along
<code>dim</code></p>
</section>
<div class="cell code" data-execution_count="32">
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> torch.tensor([<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>x[idxs]</span></code></pre></div>
<div class="output execute_result" data-execution_count="32">
<pre><code>tensor([[0, 1],
        [4, 5],
        [0, 1],
        [0, 1]])</code></pre>
</div>
</div>
<section
id="select-along-a-particular-dimension-equivalent-via-fancy-indexing-but-now-along-columns"
class="cell markdown">
<h2>Select along a particular dimension (equivalent, via fancy indexing,
but now along columns)</h2>
<p>Each entry of <code>idxs</code> corresponds to an entry along
<code>dim</code></p>
</section>
<div class="cell code" data-execution_count="33">
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>x[:, idxs]</span></code></pre></div>
<div class="output execute_result" data-execution_count="33">
<pre><code>tensor([[1, 0, 0],
        [3, 2, 2],
        [5, 4, 4]])</code></pre>
</div>
</div>
<section id="selecting-according-to-entries-of-a-mask"
class="cell markdown">
<h2>Selecting according to entries of a mask</h2>
<p><code>torch.masked_select</code> has the following signature
type:</p>
<pre><code>torch.masked_select(
    input: TensorType[D1, D2, D3, ..., DN],
    mask: TensorType[D1, D2, D3, ..., DN]
): -&gt; out: TensorType[D]</code></pre>
<p><code>out</code> will contain the entries of <code>input</code> where
<code>mask</code> is true.</p>
</section>
<div class="cell code" data-execution_count="26">
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.tensor([</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>]).<span class="bu">bool</span>()</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>x.masked_select(mask)</span></code></pre></div>
<div class="output execute_result" data-execution_count="26">
<pre><code>tensor([3, 4])</code></pre>
</div>
</div>
<section id="selecting-according-to-a-mask-but-using-fancy-indexing"
class="cell markdown">
<h2>Selecting according to a mask but using fancy indexing</h2>
<p>Use masked select if you just want a list of entries. If you want to
keep the shape of the original tensor <code>x</code>, you should use
normal masking.</p>
</section>
<div class="cell code" data-execution_count="42">
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.tensor([</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>]).<span class="bu">bool</span>()</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>x[mask]</span></code></pre></div>
<div class="output execute_result" data-execution_count="42">
<pre><code>tensor([3, 4])</code></pre>
</div>
</div>
<section id="selecting-so-that-outij--inputrowi-j-colsi-j"
class="cell markdown">
<h2>Selecting so that <code>out[i,j] = input[row[i, j], cols[i,
j]]</code></h2>
<p><code>out.shape</code> will be the same as <code>row.shape</code>.
Also <code>row.shape</code> must be equal to <code>cols.shape</code> (or
at least broadcastable, see below)</p>
</section>
<div class="cell code" data-execution_count="39">
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>rows <span class="op">=</span> torch.tensor([</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>],</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> torch.tensor([</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>x[rows, cols]</span></code></pre></div>
<div class="output execute_result" data-execution_count="39">
<pre><code>tensor([[4, 5, 4],
        [0, 1, 0]])</code></pre>
</div>
</div>
<section
id="selecting-so-that-outij--inputrowi-colsj-recommended-way-using-broadcasting"
class="cell markdown">
<h2>Selecting so that out[i,j] = input[row[i], cols[j]] (recommended way
using broadcasting)</h2>
</section>
<div class="cell code" data-execution_count="40">
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>rows <span class="op">=</span> torch.tensor([<span class="dv">2</span>, <span class="dv">0</span>])</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> torch.tensor([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>x[rows.unsqueeze(<span class="dv">1</span>), cols.unsqueeze(<span class="dv">0</span>)]</span></code></pre></div>
<div class="output execute_result" data-execution_count="40">
<pre><code>tensor([[4, 5, 4],
        [0, 1, 0]])</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Note that <code>x[rows.unsqueeze(1), cols.unsqueeze(0)]</code>
broadcasts <code>rows</code> and <code>cols</code> to be the same shape
as each other</p>
</div>
<section id="convert-a-mask-into-indices-list-of-nonzero-entries"
class="cell markdown">
<h2>Convert a mask into indices (list of nonzero entries)</h2>
</section>
<div class="cell code" data-execution_count="63">
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.tensor([</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>]).<span class="bu">bool</span>()</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>nonzeros <span class="op">=</span> mask.nonzero()</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(nonzeros)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="co"># then indexing using `nonzeros` by turning it into a pair of list of indices</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>pair <span class="op">=</span> <span class="bu">tuple</span>(nonzeros.T)</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pair)</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>x[pair]</span></code></pre></div>
<div class="output stream stdout">
<pre><code>tensor([[1, 1],
        [2, 0]])
(tensor([1, 2]), tensor([1, 0]))
</code></pre>
</div>
<div class="output execute_result" data-execution_count="63">
<pre><code>tensor([3, 4])</code></pre>
</div>
</div>
<section id="selecting-with-a-single-long-tensor" class="cell markdown">
<h2>Selecting with a single Long tensor</h2>
</section>
<div class="cell markdown">
<p>Each value in <code>tsr</code> will be read as an entry along dim 0
of <code>x</code>, so that the resulting shape is <code>(*tsr.shape,
x.shape[1:])</code>.</p>
</div>
<div class="cell code" data-execution_count="67">
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>tsr <span class="op">=</span> torch.tensor([</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">2</span>, <span class="dv">0</span>],</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>x[tsr]</span></code></pre></div>
<div class="output execute_result" data-execution_count="67">
<pre><code>tensor([[[2, 3],
         [2, 3]],

        [[4, 5],
         [0, 1]]])</code></pre>
</div>
</div>
<section
id="selecting-with-a-single-long-tensor-equivalent-but-dont-do-this"
class="cell markdown" data-tags="[]">
<h2>Selecting with a single Long tensor (equivalent, but don't do
this!)</h2>
</section>
<div class="cell code" data-execution_count="68">
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>x[tsr.flatten()].reshape(<span class="op">*</span>tsr.shape, x.shape[<span class="dv">1</span>])</span></code></pre></div>
<div class="output execute_result" data-execution_count="68">
<pre><code>tensor([[[2, 3],
         [2, 3]],

        [[4, 5],
         [0, 1]]])</code></pre>
</div>
</div>
<section id="indexing-with-slices--and-ellipsises-"
class="cell markdown" data-tags="[]">
<h2>Indexing with slices <code>:</code> and ellipsises
<code>...</code></h2>
<p>Note that pytorch may be slower with slicing</p>
</section>
<section id="indexing-with-slicing" class="cell markdown"
data-tags="[]">
<h3>Indexing with slicing</h3>
</section>
<div class="cell code" data-execution_count="76">
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>x[:, idxs]</span></code></pre></div>
<div class="output execute_result" data-execution_count="76">
<pre><code>tensor([[1, 0, 1, 0, 0],
        [3, 2, 3, 2, 2],
        [5, 4, 5, 4, 4]])</code></pre>
</div>
</div>
<div class="cell markdown">
<p>This is equivalent to indexing with a tuple of indexing arrays that
get broadcasted. Think about slicing as equivalent to indexing with
<code>torch.arange</code></p>
</div>
<div class="cell code" data-execution_count="81">
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>x[torch.arange(x.size(<span class="dv">0</span>)).unsqueeze(<span class="dv">1</span>), torch.tensor(idxs).unsqueeze(<span class="dv">0</span>)]</span></code></pre></div>
<div class="output execute_result" data-execution_count="81">
<pre><code>tensor([[1, 0, 1, 0, 0],
        [3, 2, 3, 2, 2],
        [5, 4, 5, 4, 4]])</code></pre>
</div>
</div>
<section id="indexing-using-gather" class="cell markdown"
data-tags="[]">
<h3>Indexing using gather</h3>
<p>Think about gather as a multidimensional version of selecting with
broadcasting a tuple of indexing arrays. Consider:</p>
<div class="sourceCode" id="cb48"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>x.shape <span class="op">==</span> (N x D)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>x[torch.arange(N).unsqueeze(<span class="dv">1</span>), torch.tensor([<span class="dv">1</span>, <span class="dv">2</span>].unsqueeze(<span class="dv">0</span>))]</span></code></pre></div>
<p>Here we gather along <code>dim==1</code>, resulting in an output
shape of <code>N x 2</code></p>
</section>
<div class="cell markdown">
<p>Now suppose we had a <code>B x T x D</code> tensor representing a
batch of RNN outputs each with dimension <code>D</code> Let's say we
wanted to select only the last timestep of each sequence of each batch
and that we knew how long each sequence was for each batch. We can do
the following:</p>
</div>
<div class="cell code" data-execution_count="108">
<div class="sourceCode" id="cb49"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>tsr <span class="op">=</span> torch.tensor([</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">1.</span>, <span class="fl">2.</span>, <span class="fl">3.</span>, <span class="fl">4.</span>],</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">5.</span>, <span class="fl">6.</span>, <span class="fl">7.</span>, <span class="fl">8.</span>],</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.</span>, <span class="fl">0.</span>, <span class="fl">0.</span>, <span class="fl">0.</span>],</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">9.</span>, <span class="fl">8.</span>, <span class="fl">7.</span>, <span class="fl">6.</span>],</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.</span>, <span class="fl">0.</span>, <span class="fl">0.</span>, <span class="fl">0.</span>],</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.</span>, <span class="fl">0.</span>, <span class="fl">0.</span>, <span class="fl">0.</span>],</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>]) <span class="co"># use 0 to indicate padding</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>N, T, D <span class="op">=</span> tsr.shape</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>lengths <span class="op">=</span> torch.tensor([<span class="dv">2</span>, <span class="dv">1</span>])  <span class="co"># length of each of the N sequences</span></span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>last_idxs <span class="op">=</span> lengths <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> last_idxs.unsqueeze(<span class="op">-</span><span class="dv">1</span>).expand(N, D).unsqueeze(<span class="op">-</span><span class="dv">2</span>)  <span class="co"># B x 1 x D</span></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(index)</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>tsr.gather(index<span class="op">=</span>index, dim<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>tensor([[[1, 1, 1, 1]],

        [[0, 0, 0, 0]]])
</code></pre>
</div>
<div class="output execute_result" data-execution_count="108">
<pre><code>tensor([[[5., 6., 7., 8.]],

        [[9., 8., 7., 6.]]])</code></pre>
</div>
</div>
<div class="cell markdown">
<p>The output size will be the same as <code>index</code> size in all
dimensions except <code>dim</code>. <code>index</code> must have the
same size in all dimensions as <code>tsr</code> except dimension
<code>dim</code>. <code>dim</code> specifies the dimension along which
the values in <code>index</code> will be indexing</p>
</div>
<section id="indexing-timings" class="cell markdown" data-tags="[]">
<h3>Indexing timings</h3>
<p>Let's see which of these equivalent gather methods are faster!</p>
</section>
<div class="cell code" data-execution_count="50">
<div class="sourceCode" id="cb52"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_gather(tensor, indices):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> []</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(tensor.size(<span class="dv">0</span>)):</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>        output <span class="op">+=</span> [tensor[i][indices[i]]]</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.stack(output)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="51">
<div class="sourceCode" id="cb53"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_gather_vec(tensor, indices):</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    shape <span class="op">=</span> <span class="bu">list</span>(tensor.shape)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    flat_first <span class="op">=</span> torch.reshape(</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>        tensor, [shape[<span class="dv">0</span>] <span class="op">*</span> shape[<span class="dv">1</span>]] <span class="op">+</span> shape[<span class="dv">2</span>:])</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">=</span> torch.reshape(</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>        torch.arange(shape[<span class="dv">0</span>], device<span class="op">=</span>tensor.device) <span class="op">*</span> shape[<span class="dv">1</span>],</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>        [shape[<span class="dv">0</span>]] <span class="op">+</span> [<span class="dv">1</span>] <span class="op">*</span> (<span class="bu">len</span>(indices.shape) <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> flat_first[indices <span class="op">+</span> offset]</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="52">
<div class="sourceCode" id="cb54"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">&quot;cuda&quot;</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1000</span>, <span class="dv">200</span>, device<span class="op">=</span>device)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> torch.randint(<span class="dv">200</span>, (<span class="dv">1000</span>, ), device<span class="op">=</span>device)</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>idx_lst <span class="op">=</span> idx.tolist()</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="65">
<div class="sourceCode" id="cb55"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>batch_gather(x, idx).<span class="bu">sum</span>()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>10.6 ms ± 36.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="67">
<div class="sourceCode" id="cb57"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>batch_gather_vec(x, idx).<span class="bu">sum</span>()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>51.4 µs ± 1.2 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="66">
<div class="sourceCode" id="cb59"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>x.gather(index<span class="op">=</span>idx.unsqueeze(<span class="op">-</span><span class="dv">1</span>), dim<span class="op">=-</span><span class="dv">1</span>).squeeze(<span class="op">-</span><span class="dv">1</span>).<span class="bu">sum</span>()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>19.6 µs ± 332 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Clearly the gather function built into pytorch is the fastest.</p>
<p>How about the timing for summing along an axis/dimension?</p>
</div>
<div class="cell code" data-execution_count="56">
<div class="sourceCode" id="cb61"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x)):</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res <span class="op">+</span> x[i]</span></code></pre></div>
<div class="output stream stdout">
<pre><code>6.95 ms ± 50.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="57">
<div class="sourceCode" id="cb63"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> row <span class="kw">in</span> x:</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res <span class="op">+</span> row</span></code></pre></div>
<div class="output stream stdout">
<pre><code>5 ms ± 39.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="58">
<div class="sourceCode" id="cb65"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> row <span class="kw">in</span> x.unbind():</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res <span class="op">+</span> row</span></code></pre></div>
<div class="output stream stdout">
<pre><code>4.99 ms ± 26.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="59">
<div class="sourceCode" id="cb67"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>x.<span class="bu">sum</span>(<span class="dv">0</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>15 µs ± 17.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>What about indexing?</p>
</div>
<div class="cell code" data-execution_count="60">
<div class="sourceCode" id="cb69"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>x[idx_lst]</span></code></pre></div>
<div class="output stream stdout">
<pre><code>54.3 µs ± 568 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="61">
<div class="sourceCode" id="cb71"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>x.index_select(index<span class="op">=</span>idx, dim<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>7.03 µs ± 9.33 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="68">
<div class="sourceCode" id="cb73"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>x[idx]</span></code></pre></div>
<div class="output stream stdout">
<pre><code>9.39 µs ± 62.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
</code></pre>
</div>
</div>
</article>
<script src="https://giscus.app/client.js"
        data-repo="KeAWang/pensieve"
        data-repo-id="R_kgDOGwh2Sg"
        data-category="Announcements"
        data-category-id="DIC_kwDOGwh2Ss4CA-nB"
        data-mapping="pathname"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</body>
</html>
